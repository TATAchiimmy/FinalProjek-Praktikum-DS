---
title: "R Notebook"
output: html_document
---
# Library
```{r}
library(twitteR) # scrapping
library(tm) # corpus
library(syuzhet) # labeling
library(caTools) # split data
library(dplyr) # case when

set.seed(100)
```

# Setting API Twitter
```{r}
consumer_key <- "YTkbdeL6EymZy1HJp6tiPxUEr"
consumer_secret <- "0VoTq25uP4DTjnzKNaBhxcCoaSbdvAlfqsviZjd50iCinRj5C4"
bearer_token <- "AAAAAAAAAAAAAAAAAAAAAD9UjAEAAAAA8m3PgMs24tRhYCuJ0hQaQ5R2moI%3DThYb3MRjCOlrHidGcDbn4ZDdOG0NtwbeZuY3IDFk0ay8Vdpla5"
access_token <- "1437206819936501760-zvrXApLJ4MRUDd5zwSDmKXYFTbJ7eV"
access_token_secret <- "0n6bGTCeEQUYDnwaImK0Big8ZhIMtIxwF2IYvXiRPiV20"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_token_secret)
```
# Scraping
```{r}

# Scrapping data (Ambil data dari twitter)
tweetsList <- searchTwitter('Uber', n = 1000, retryOnRateLimit = 10e5, lang = "en")
# Mengubah data list twitter menjadi data frame
tweets <- twListToDF(twList = tweetsList)
write.csv(tweets, "data_tweet.csv")
```

# Data Cleaning
```{r}
# remove spam tweets
uniqueText <- unique(tweets$text)

# remove retweet element
removeRetweet <- function(x) gsub("RT @\\w+: ", "", x)
cleanText <- lapply(uniqueText, removeRetweet)

#remove mentione element
removeMention <- function(x) gsub("@\\w+", "", x)
cleanText <- lapply(cleanText, removeMention)

# remove url element
removeURL <- function(x) gsub("http\\S+", "", x)
cleanText <- lapply(cleanText, removeURL)

# remove hastag element
removeHashtag <- function(x) gsub("#\\S+", "", x)
cleanText <- lapply(cleanText, removeHashtag)

# remove new line character
removeNewLine <- function(x) gsub("\n", " ", x)
cleanText <- lapply(cleanText, removeNewLine)

# remove nonalphabetical character
removeNonAlphabet <- function(x) gsub("[^A-Za-z ]", "", x)
cleanText <- lapply(cleanText, removeNonAlphabet)

# trim space into one space
cleanText <- lapply(cleanText, stripWhitespace)

# text to lowecase
cleanText <- lapply(cleanText, tolower)

# remove stop words
cleanText <- lapply(cleanText, removeWords, stopwords("english"))
dataframe <- data.frame(tweet = unlist(cleanText))
write.csv(dataframe, "data_clean.csv")
```

# Labeling

```{r}
positiveWords <- scan("positive.txt", what = "character", comment.char = ";")
negativeWords <- scan("negative.txt", what = "character", comment.char = ";")

# menentukan score tweet
scores <- lapply(cleanText, function(cleanText) {
  words <- unlist(str_split(cleanText, pattern = "\\s+"))
  positiveMatches <- !is.na(match(words, positiveWords))
  negativeMatches <- !is.na(match(words, negativeWords))
  score <- sum(positiveMatches) - sum(negativeMatches)
  score
})

# melabeli jika score -1 maka negatif, jika score 0 maka netral, jika score 1 maka positif
sentiment <- as.factor(ifelse(scores < 0, "negative", ifelse(scores == 0, "neutral", "positive")))
```
# Build csv
```{r}
# menyimpan data yang sudah dilabeli ke dalam file csv
dataframe <- data.frame(tweet = unlist(cleanText), sentiment = sentiment)
write.csv(dataframe, "data_sentiment.csv")
```